# Overall Purpose: This script lets users load the raw messy data, clean up the data usingsequential cleaning steps, and then store the finalized clean version into the processed file. The cleaning steps are standardizing column names, cleaning up strings, handling missing quantities/prices, and removing rows with invalid values.

import re
from pathlib import Path
import pandas as pd
from typing import Any

# Step 1: Standardize Column Names
# Convert to snake_case, make sure there's no unneccessary symbols, remove characters that are not numbers, letters, or underscores)
# What changes were made to the og function generated by CoPilot? 
# - Made simpler by author 
# - Removed unicode and ASCII section (deemed to complicated for this project)
# - Rewrote comments and replaced variable names for simplicity and added clarity for users

def standardize_column_name(name: Any) -> str:
	# Return "col" for column types with no names
	if name is None:
		return "col"
	
    # Convert og names to strings; stripping leading whitespace
	original_name = str(name).strip()

	# Insert underscore between lower-case and upper-case letters
	with_underscores = re.sub(r"([a-z0-9])([A-Z])", r"\1_\2", original_name)

	# Replacing anything with underscores (seperators such as spaces and hypens)
	replacements = re.sub(r"[\s\-]+", "_", with_underscores)

	# Lowercase name 
	lowercased = replacements.lower()
	
	# Empty names are replaced by "col"; renamed to clean_column by author for clarity
	if lowercased == "":
		lowercased = "col"

	return lowercased


def clean_column_names(df: pd.DataFrame, inplace: bool = False) -> pd.DataFrame:
	# Step 2: Clean Column Values
	# Remove quotes, trim whitespace, and fix formatting
	original_columns = list(df.columns)
	new = [standardize_column_name(column) for column in original_columns]

	seen = set()
	unique = []
	# Make column names unique; Author modifications include variable renaming (replacing 'i' with 'count' for clarity and code-style preference)
	for base in new:
		candidate = base
		count = 1
		while candidate in seen:
			candidate = f"{base}_{count}"
			count += 1
		seen.add(candidate)
		unique.append(candidate)

	if inplace:
		df.columns = unique
		return df
	return df.rename(columns=dict(zip(original_columns, unique)))


def load_data(raw_path: str) -> pd.DataFrame:
	df = pd.read_csv(raw_path)
	df = clean_column_names(df)
	return df


def clean_string(value, strip_quotes=True, collapse_spaces=True, title_case=False):
	if pd.isna(value):
		return value
	cleaned_value = str(value)
	if strip_quotes:
		cleaned_value = cleaned_value.replace('"', '').replace("'", '')
	if collapse_spaces:
		cleaned_value = ' '.join(cleaned_value.split())
	cleaned_value = cleaned_value.strip()
	if cleaned_value == '':
		return pd.NA
	if title_case:
		cleaned_value = cleaned_value.title()
	return cleaned_value

# Step 3: Handle missing prices & quantities
def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:
	# Clean product names and categories, converting price and quantity to numeric types
	if 'prod_name' in df.columns:
		df['prod_name'] = df['prod_name'].apply(lambda v: clean_string(v, strip_quotes=True, collapse_spaces=True, title_case=False))

	if 'category' in df.columns:
		df['category'] = df['category'].apply(lambda v: clean_string(v, strip_quotes=True, collapse_spaces=True, title_case=True))
    
	if 'price' in df.columns:
		df['price'] = pd.to_numeric(df['price'], errors='coerce')

	if 'qty' in df.columns:
		df['qty'] = pd.to_numeric(df['qty'], errors='coerce')

	return df

# Step 4: Remove Rows with Clearly Invalid Values
def remove_invalid_rows(df: pd.DataFrame) -> pd.DataFrame:
	# Drop rows where price or quantity are negative, move them to the audit file
	# Why did I accept the audit file idea proposed by CoPilot?
	# - Great for future reference if someone needed to see the data; works especially for financial data 
	invalid_mask = pd.Series(False, index=df.index)
	if 'price' in df.columns:
		invalid_mask = invalid_mask | (df['price'] < 0)
	if 'qty' in df.columns:
		invalid_mask = invalid_mask | (df['qty'] < 0)
	if invalid_mask.any():
		invalid = int(invalid_mask.sum())
		print(f"\nDropping {invalid} rows that have any negative price or quantity!")

		dropped = df.loc[invalid_mask].copy()
       
        # This function lets one know why each row was dropped 
		def _reason(row):
			reasons = []
			if 'price' in row.index and pd.notna(row['price']) and row['price'] < 0:
				reasons.append('negative_price')
			if 'qty' in row.index and pd.notna(row['qty']) and row['qty'] < 0:
				reasons.append('negative_qty')
			# Returning reasons why row was dropped; joined w/ a semicolon if there is more than one reason 
			return ';'.join(reasons) if reasons else 'unknown' 

		dropped['drop_reason'] = dropped.apply(_reason, axis=1)

        # Save dropped rows to audit file (located in processed directory)
		# Using mkdir to make sure that the processed directory exists
		# No changes made by author other than comments and clarifying print statement
		processed_dir = Path('data/processed')
		processed_dir.mkdir(parents=True, exist_ok=True)
		audit_path = processed_dir / 'dropped_rows_audit.csv'
		dropped.to_csv(audit_path, index=False)
		print(f"Saved dropped rows audit to: {audit_path} for additonal reference!")
        
        # Remove invalid rows from og dataframe 
		df = df.loc[~invalid_mask].reset_index(drop=True)
	else: 
		# No invalid rows were found
		print("\nNo negative price/qty rows exist! No rows dropped.")

	return df

# Main execution block necessary for assignment 
# Ensures that the script runs properly and executes data cleaning sequentially
if __name__ == "__main__":
	raw_path = "data/raw/sales_data_raw.csv"
	cleaned_path = "data/processed/sales_data_clean.csv"

	df_raw = load_data(raw_path)
	df_clean = handle_missing_values(df_raw)
	df_clean = remove_invalid_rows(df_clean)
	df_clean.to_csv(cleaned_path, index=False)
	print("Cleaning complete. First few rows:")
	print(df_clean.head())



